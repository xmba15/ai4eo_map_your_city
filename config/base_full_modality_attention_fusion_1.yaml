---
seed: 2024

num_workers: 4
experiment_name: "2024-07-14-f9-attention-fusion"

dataset:
  n_splits: 10
  fold_th: 9
  train_dir: ~/publicWorkspace/data/building-age-dataset/train/data
  test_dir: ~/publicWorkspace/data/building-age-dataset/test/data
  train_csv: ~/publicWorkspace/data/building-age-dataset/train/train-set.csv
  test_csv: ~/publicWorkspace/data/building-age-dataset/test/test-set.csv

model:
  type: src.models.MultiModalNetFullModalityAttentionFusion
  encoder_name: tf_efficientnetv2_b3
  num_classes: 7

loss:
  classification:
    type: src.models.FocalLossLabelSmoothing

optimizer:
  type: timm.optim.AdamW
  lr: 0.0001
  weight_decay: 0.001

scheduler:
  type: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 30
  eta_min: 0.00005

trainer:
  devices: [0]
  accelerator: "cuda"
  max_epochs: 30
  gradient_clip_val: 5.0
  accumulate_grad_batches: 8
  resume_from_checkpoint:

train_parameters:
  batch_size: 6

val_parameters:
  batch_size: 6

output_root_dir: experiments
image_size: 512
